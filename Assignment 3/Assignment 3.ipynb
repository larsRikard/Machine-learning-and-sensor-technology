{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code source: https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"Report\", \"fig\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression MLP imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, max_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers \"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for making models and generating plots, stolen from previous solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X_train, y_train, X_test, y_test, model):\n",
    " \n",
    "    y_train_predicted = model.predict(X_train)\n",
    "    y_test_predicted = model.predict(X_test)\n",
    "    \n",
    "    print('--------------------TRAIN SET-------------------------')\n",
    "    print('r2_score = ','%.2f' % r2_score(y_train,y_train_predicted), '(1.0 means perfect fit)')\n",
    "    print('max_error = ','%.2f' % max_error(y_train,y_train_predicted))\n",
    "    print('root_mean_squared_error = ','%.2f' % mean_squared_error(y_train,y_train_predicted, squared=False))\n",
    "    print('--------------------TEST SET-------------------------')\n",
    "    print('r2_score = ','%.2f' % r2_score(y_test,y_test_predicted), '(1.0 means perfect fit)')\n",
    "    print('max_error = ','%.2f' % max_error(y_test,y_test_predicted))\n",
    "    print('root_mean_squared_error = ','%.2f' % mean_squared_error(y_test,y_test_predicted, squared=False))\n",
    "    print(y_train.index[:100])\n",
    "    \n",
    "    return r2_score(y_train,y_train_predicted), r2_score(y_test,y_test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the training set and the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' fan_prepared_test = num_pipeline.fit_transform(fd_001_test)\\nfan_labels_test = fd_001_test[\"RUL\"].copy() '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "useless_features = ['engine','RUL', 'cycle', 'setting 3', 'sensor 1', 'sensor 5', 'sensor 6', 'sensor 10','sensor 14', 'sensor 16', 'sensor 18', 'sensor 19']\n",
    "\n",
    "fd_001_train  = pd.read_csv('train_FD001.csv')\n",
    "y_train_full = fd_001_train['RUL']\n",
    "X_train_full = fd_001_train.drop(columns=useless_features)\n",
    "\n",
    "fd_001_test = pd.read_csv('test_FD001.csv')\n",
    "y_test = fd_001_test['RUL']\n",
    "X_test = fd_001_test.drop(columns=useless_features)\n",
    "\n",
    "\"\"\" X_mlp_train_full = X_train.copy()\n",
    "y_mlp_train_full = y_train.copy()\n",
    "X_mlp_test = X_train.copy()\n",
    "y_mlp_test = y_train.copy() \"\"\"\n",
    "\n",
    "\"\"\" X_mlp_train, X_mlp_valid, y_mlp_train, y_mlp_valid = train_test_split(\n",
    "    X_mlp_train_full, y_mlp_train_full, random_state=42) \"\"\"\n",
    "\n",
    "\"\"\" fan_prepared_test = num_pipeline.fit_transform(fd_001_test)\n",
    "fan_labels_test = fd_001_test[\"RUL\"].copy() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setting 1</th>\n",
       "      <th>setting 2</th>\n",
       "      <th>sensor 2</th>\n",
       "      <th>sensor 3</th>\n",
       "      <th>sensor 4</th>\n",
       "      <th>sensor 7</th>\n",
       "      <th>sensor 8</th>\n",
       "      <th>sensor 9</th>\n",
       "      <th>sensor 11</th>\n",
       "      <th>sensor 12</th>\n",
       "      <th>sensor 13</th>\n",
       "      <th>sensor 15</th>\n",
       "      <th>sensor 17</th>\n",
       "      <th>sensor 20</th>\n",
       "      <th>sensor 21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>642.680934</td>\n",
       "      <td>1590.523119</td>\n",
       "      <td>1408.933782</td>\n",
       "      <td>553.367711</td>\n",
       "      <td>2388.096652</td>\n",
       "      <td>9065.242941</td>\n",
       "      <td>47.541168</td>\n",
       "      <td>521.413470</td>\n",
       "      <td>2388.096152</td>\n",
       "      <td>8.442146</td>\n",
       "      <td>393.210654</td>\n",
       "      <td>38.816271</td>\n",
       "      <td>23.289705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.500053</td>\n",
       "      <td>6.131150</td>\n",
       "      <td>9.000605</td>\n",
       "      <td>0.885092</td>\n",
       "      <td>0.070985</td>\n",
       "      <td>22.082880</td>\n",
       "      <td>0.267087</td>\n",
       "      <td>0.737553</td>\n",
       "      <td>0.071919</td>\n",
       "      <td>0.037505</td>\n",
       "      <td>1.548763</td>\n",
       "      <td>0.180746</td>\n",
       "      <td>0.108251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.008700</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>641.210000</td>\n",
       "      <td>1571.040000</td>\n",
       "      <td>1382.250000</td>\n",
       "      <td>549.850000</td>\n",
       "      <td>2387.900000</td>\n",
       "      <td>9021.730000</td>\n",
       "      <td>46.850000</td>\n",
       "      <td>518.690000</td>\n",
       "      <td>2387.880000</td>\n",
       "      <td>8.324900</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>38.140000</td>\n",
       "      <td>22.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.001500</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>642.325000</td>\n",
       "      <td>1586.260000</td>\n",
       "      <td>1402.360000</td>\n",
       "      <td>552.810000</td>\n",
       "      <td>2388.050000</td>\n",
       "      <td>9053.100000</td>\n",
       "      <td>47.350000</td>\n",
       "      <td>520.960000</td>\n",
       "      <td>2388.040000</td>\n",
       "      <td>8.414900</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>38.700000</td>\n",
       "      <td>23.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>642.640000</td>\n",
       "      <td>1590.100000</td>\n",
       "      <td>1408.040000</td>\n",
       "      <td>553.440000</td>\n",
       "      <td>2388.090000</td>\n",
       "      <td>9060.660000</td>\n",
       "      <td>47.510000</td>\n",
       "      <td>521.480000</td>\n",
       "      <td>2388.090000</td>\n",
       "      <td>8.438900</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>38.830000</td>\n",
       "      <td>23.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>643.000000</td>\n",
       "      <td>1594.380000</td>\n",
       "      <td>1414.555000</td>\n",
       "      <td>554.010000</td>\n",
       "      <td>2388.140000</td>\n",
       "      <td>9069.420000</td>\n",
       "      <td>47.700000</td>\n",
       "      <td>521.950000</td>\n",
       "      <td>2388.140000</td>\n",
       "      <td>8.465600</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>38.950000</td>\n",
       "      <td>23.366800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>644.530000</td>\n",
       "      <td>1616.910000</td>\n",
       "      <td>1441.490000</td>\n",
       "      <td>556.060000</td>\n",
       "      <td>2388.560000</td>\n",
       "      <td>9244.590000</td>\n",
       "      <td>48.530000</td>\n",
       "      <td>523.380000</td>\n",
       "      <td>2388.560000</td>\n",
       "      <td>8.584800</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>39.430000</td>\n",
       "      <td>23.618400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          setting 1     setting 2      sensor 2      sensor 3      sensor 4  \\\n",
       "count  20631.000000  20631.000000  20631.000000  20631.000000  20631.000000   \n",
       "mean      -0.000009      0.000002    642.680934   1590.523119   1408.933782   \n",
       "std        0.002187      0.000293      0.500053      6.131150      9.000605   \n",
       "min       -0.008700     -0.000600    641.210000   1571.040000   1382.250000   \n",
       "25%       -0.001500     -0.000200    642.325000   1586.260000   1402.360000   \n",
       "50%        0.000000      0.000000    642.640000   1590.100000   1408.040000   \n",
       "75%        0.001500      0.000300    643.000000   1594.380000   1414.555000   \n",
       "max        0.008700      0.000600    644.530000   1616.910000   1441.490000   \n",
       "\n",
       "           sensor 7      sensor 8      sensor 9     sensor 11     sensor 12  \\\n",
       "count  20631.000000  20631.000000  20631.000000  20631.000000  20631.000000   \n",
       "mean     553.367711   2388.096652   9065.242941     47.541168    521.413470   \n",
       "std        0.885092      0.070985     22.082880      0.267087      0.737553   \n",
       "min      549.850000   2387.900000   9021.730000     46.850000    518.690000   \n",
       "25%      552.810000   2388.050000   9053.100000     47.350000    520.960000   \n",
       "50%      553.440000   2388.090000   9060.660000     47.510000    521.480000   \n",
       "75%      554.010000   2388.140000   9069.420000     47.700000    521.950000   \n",
       "max      556.060000   2388.560000   9244.590000     48.530000    523.380000   \n",
       "\n",
       "          sensor 13     sensor 15     sensor 17     sensor 20     sensor 21  \n",
       "count  20631.000000  20631.000000  20631.000000  20631.000000  20631.000000  \n",
       "mean    2388.096152      8.442146    393.210654     38.816271     23.289705  \n",
       "std        0.071919      0.037505      1.548763      0.180746      0.108251  \n",
       "min     2387.880000      8.324900    388.000000     38.140000     22.894200  \n",
       "25%     2388.040000      8.414900    392.000000     38.700000     23.221800  \n",
       "50%     2388.090000      8.438900    393.000000     38.830000     23.297900  \n",
       "75%     2388.140000      8.465600    394.000000     38.950000     23.366800  \n",
       "max     2388.560000      8.584800    400.000000     39.430000     23.618400  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Multi-layer Perceptron</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 hidden layers, Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42)\n",
    "pipeline_mlp_1 = make_pipeline(StandardScaler(), mlp_reg)\n",
    "pipeline_mlp_1.fit(X_train_full, y_train_full)\n",
    "y_pred = pipeline_mlp_1.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.594817064172354"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 hidden layers, lbfgs optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\larsr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42, solver='lbfgs')\n",
    "pipeline_mlp_2 = make_pipeline(StandardScaler(), mlp_reg)\n",
    "pipeline_mlp_2.fit(X_train_full, y_train_full)\n",
    "y_pred = pipeline_mlp_2.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.553188000035384"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam optimizer with lower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42, learning_rate_init=0.0005, max_iter=600)\n",
    "pipeline_mlp_3 = make_pipeline(StandardScaler(), mlp_reg)\n",
    "pipeline_mlp_3.fit(X_train_full, y_train_full)\n",
    "y_pred = pipeline_mlp_3.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------TRAIN SET-------------------------\n",
      "r2_score =  0.65 (1.0 means perfect fit)\n",
      "max_error =  223.69\n",
      "root_mean_squared_error =  40.63\n",
      "--------------------TEST SET-------------------------\n",
      "r2_score =  0.38 (1.0 means perfect fit)\n",
      "max_error =  218.75\n",
      "root_mean_squared_error =  46.59\n",
      "RangeIndex(start=0, stop=100, step=1)\n",
      "--------------------TRAIN SET-------------------------\n",
      "r2_score =  0.67 (1.0 means perfect fit)\n",
      "max_error =  212.16\n",
      "root_mean_squared_error =  39.70\n",
      "--------------------TEST SET-------------------------\n",
      "r2_score =  0.35 (1.0 means perfect fit)\n",
      "max_error =  228.70\n",
      "root_mean_squared_error =  47.55\n",
      "RangeIndex(start=0, stop=100, step=1)\n",
      "--------------------TRAIN SET-------------------------\n",
      "r2_score =  0.66 (1.0 means perfect fit)\n",
      "max_error =  217.51\n",
      "root_mean_squared_error =  40.05\n",
      "--------------------TEST SET-------------------------\n",
      "r2_score =  0.38 (1.0 means perfect fit)\n",
      "max_error =  214.91\n",
      "root_mean_squared_error =  46.47\n",
      "RangeIndex(start=0, stop=100, step=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6619856937750026, 0.379175645112903)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(X_train_full,y_train_full,X_test,y_test, pipeline_mlp_1)\n",
    "test_model(X_train_full,y_train_full,X_test,y_test, pipeline_mlp_2)\n",
    "test_model(X_train_full,y_train_full,X_test,y_test, pipeline_mlp_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Recurrent Neural Network</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to load the dataset with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets, scale_data=True):\n",
    "        if scale_data:\n",
    "            data_standard = StandardScaler().fit_transform(data)\n",
    "        self.X = torch.from_numpy(data_standard)\n",
    "        self.y = torch.tensor(targets.values, dtype=torch.float)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        current_data = self.X[idx, :]\n",
    "        current_target = self.y[idx]\n",
    "        sample = current_data,  current_target\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(data=X_train_full, targets=y_train_full)\n",
    "test_dataset = CustomDataset(data=X_test, targets=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.3160, -1.3730, -1.7217, -0.1343, -0.9259,  1.1211, -0.5163, -0.8628,\n",
       "         -0.2665,  0.3343, -1.0589, -0.6038, -0.7817,  1.3485,  1.1944],\n",
       "        dtype=torch.float64),\n",
       " tensor(191.))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset)\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi layer perceptron with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron for regression.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(15, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "      Forward pass\n",
    "    '''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (data):\n",
    "    # Get and prepare inputs\n",
    "    inputs, targets = data\n",
    "    inputs, targets = inputs.float(), targets.float()\n",
    "    targets = targets.reshape((targets.shape[0], 1))\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Perform forward pass\n",
    "    outputs = mlp(inputs)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function(outputs, targets)\n",
    "    \n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Perform optimization\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test (data):\n",
    "     # Get and prepare inputs\n",
    "    inputs, targets = data\n",
    "    inputs, targets = inputs.float(), targets.float()\n",
    "    targets = targets.reshape((targets.shape[0], 1))\n",
    "    \n",
    "    # Perform forward pass\n",
    "    outputs = mlp(inputs)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function(outputs, targets)\n",
    "    \n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 0.063\n",
      "Loss after mini-batch  1001: 60.612\n",
      "Loss after mini-batch  2001: 43.916\n",
      "Loss after mini-batch  3001: 37.899\n",
      "Loss after mini-batch  4001: 69.807\n",
      "Loss after mini-batch  5001: 48.674\n",
      "Loss after mini-batch  6001: 43.942\n",
      "Loss after mini-batch  7001: 46.107\n",
      "Loss after mini-batch  8001: 48.070\n",
      "Loss after mini-batch  9001: 49.744\n",
      "Loss after mini-batch 10001: 44.171\n",
      "Loss after mini-batch 11001: 45.087\n",
      "Loss after mini-batch 12001: 69.970\n",
      "Loss after mini-batch 13001: 83.329\n",
      "Loss after mini-batch 14001: 123.782\n",
      "Loss after mini-batch 15001: 52.020\n",
      "Loss after mini-batch 16001: 46.966\n",
      "Loss after mini-batch 17001: 73.061\n",
      "Loss after mini-batch 18001: 64.479\n",
      "Loss after mini-batch 19001: 88.230\n",
      "Loss after mini-batch 20001: 95.625\n",
      "Test loss after mini-batch     1: 0.005\n",
      "Test loss after mini-batch  1001: 122.333\n",
      "Test loss after mini-batch  2001: 131.530\n",
      "Test loss after mini-batch  3001: 64.492\n",
      "Test loss after mini-batch  4001: 115.610\n",
      "Test loss after mini-batch  5001: 101.831\n",
      "Test loss after mini-batch  6001: 136.603\n",
      "Test loss after mini-batch  7001: 84.533\n",
      "Test loss after mini-batch  8001: 96.861\n",
      "Test loss after mini-batch  9001: 77.062\n",
      "Test loss after mini-batch 10001: 102.438\n",
      "Test loss after mini-batch 11001: 98.807\n",
      "Test loss after mini-batch 12001: 134.943\n",
      "Test loss after mini-batch 13001: 152.204\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 0.063\n",
      "Loss after mini-batch  1001: 60.612\n",
      "Loss after mini-batch  2001: 43.916\n",
      "Loss after mini-batch  3001: 37.899\n",
      "Loss after mini-batch  4001: 69.807\n",
      "Loss after mini-batch  5001: 48.674\n",
      "Loss after mini-batch  6001: 43.942\n",
      "Loss after mini-batch  7001: 46.107\n",
      "Loss after mini-batch  8001: 48.070\n",
      "Loss after mini-batch  9001: 49.744\n",
      "Loss after mini-batch 10001: 44.171\n",
      "Loss after mini-batch 11001: 45.087\n",
      "Loss after mini-batch 12001: 69.970\n",
      "Loss after mini-batch 13001: 83.329\n",
      "Loss after mini-batch 14001: 123.782\n",
      "Loss after mini-batch 15001: 52.020\n",
      "Loss after mini-batch 16001: 46.966\n",
      "Loss after mini-batch 17001: 73.061\n",
      "Loss after mini-batch 18001: 64.479\n",
      "Loss after mini-batch 19001: 88.230\n",
      "Loss after mini-batch 20001: 95.625\n",
      "Test loss after mini-batch     1: 0.005\n",
      "Test loss after mini-batch  1001: 122.333\n",
      "Test loss after mini-batch  2001: 131.530\n",
      "Test loss after mini-batch  3001: 64.492\n",
      "Test loss after mini-batch  4001: 115.610\n",
      "Test loss after mini-batch  5001: 101.831\n",
      "Test loss after mini-batch  6001: 136.603\n",
      "Test loss after mini-batch  7001: 84.533\n",
      "Test loss after mini-batch  8001: 96.861\n",
      "Test loss after mini-batch  9001: 77.062\n",
      "Test loss after mini-batch 10001: 102.438\n",
      "Test loss after mini-batch 11001: 98.807\n",
      "Test loss after mini-batch 12001: 134.943\n",
      "Test loss after mini-batch 13001: 152.204\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 0.063\n",
      "Loss after mini-batch  1001: 60.612\n",
      "Loss after mini-batch  2001: 43.916\n",
      "Loss after mini-batch  3001: 37.899\n",
      "Loss after mini-batch  4001: 69.807\n",
      "Loss after mini-batch  5001: 48.674\n",
      "Loss after mini-batch  6001: 43.942\n",
      "Loss after mini-batch  7001: 46.107\n",
      "Loss after mini-batch  8001: 48.070\n",
      "Loss after mini-batch  9001: 49.744\n",
      "Loss after mini-batch 10001: 44.171\n",
      "Loss after mini-batch 11001: 45.087\n",
      "Loss after mini-batch 12001: 69.970\n",
      "Loss after mini-batch 13001: 83.329\n",
      "Loss after mini-batch 14001: 123.782\n",
      "Loss after mini-batch 15001: 52.020\n",
      "Loss after mini-batch 16001: 46.966\n",
      "Loss after mini-batch 17001: 73.061\n",
      "Loss after mini-batch 18001: 64.479\n",
      "Loss after mini-batch 19001: 88.230\n",
      "Loss after mini-batch 20001: 95.625\n",
      "Test loss after mini-batch     1: 0.005\n",
      "Test loss after mini-batch  1001: 122.333\n",
      "Test loss after mini-batch  2001: 131.530\n",
      "Test loss after mini-batch  3001: 64.492\n",
      "Test loss after mini-batch  4001: 115.610\n",
      "Test loss after mini-batch  5001: 101.831\n",
      "Test loss after mini-batch  6001: 136.603\n",
      "Test loss after mini-batch  7001: 84.533\n",
      "Test loss after mini-batch  8001: 96.861\n",
      "Test loss after mini-batch  9001: 77.062\n",
      "Test loss after mini-batch 10001: 102.438\n",
      "Test loss after mini-batch 11001: 98.807\n",
      "Test loss after mini-batch 12001: 134.943\n",
      "Test loss after mini-batch 13001: 152.204\n",
      "Starting epoch 4\n",
      "Loss after mini-batch     1: 0.063\n",
      "Loss after mini-batch  1001: 60.612\n",
      "Loss after mini-batch  2001: 43.916\n",
      "Loss after mini-batch  3001: 37.899\n",
      "Loss after mini-batch  4001: 69.807\n",
      "Loss after mini-batch  5001: 48.674\n",
      "Loss after mini-batch  6001: 43.942\n",
      "Loss after mini-batch  7001: 46.107\n",
      "Loss after mini-batch  8001: 48.070\n",
      "Loss after mini-batch  9001: 49.744\n",
      "Loss after mini-batch 10001: 44.171\n",
      "Loss after mini-batch 11001: 45.087\n",
      "Loss after mini-batch 12001: 69.970\n",
      "Loss after mini-batch 13001: 83.329\n",
      "Loss after mini-batch 14001: 123.782\n",
      "Loss after mini-batch 15001: 52.020\n",
      "Loss after mini-batch 16001: 46.966\n",
      "Loss after mini-batch 17001: 73.061\n",
      "Loss after mini-batch 18001: 64.479\n",
      "Loss after mini-batch 19001: 88.230\n",
      "Loss after mini-batch 20001: 95.625\n",
      "Test loss after mini-batch     1: 0.005\n",
      "Test loss after mini-batch  1001: 122.333\n",
      "Test loss after mini-batch  2001: 131.530\n",
      "Test loss after mini-batch  3001: 64.492\n",
      "Test loss after mini-batch  4001: 115.610\n",
      "Test loss after mini-batch  5001: 101.831\n",
      "Test loss after mini-batch  6001: 136.603\n",
      "Test loss after mini-batch  7001: 84.533\n",
      "Test loss after mini-batch  8001: 96.861\n",
      "Test loss after mini-batch  9001: 77.062\n",
      "Test loss after mini-batch 10001: 102.438\n",
      "Test loss after mini-batch 11001: 98.807\n",
      "Test loss after mini-batch 12001: 134.943\n",
      "Test loss after mini-batch 13001: 152.204\n",
      "Starting epoch 5\n",
      "Loss after mini-batch     1: 0.063\n",
      "Loss after mini-batch  1001: 60.612\n",
      "Loss after mini-batch  2001: 43.916\n",
      "Loss after mini-batch  3001: 37.899\n",
      "Loss after mini-batch  4001: 69.807\n",
      "Loss after mini-batch  5001: 48.674\n",
      "Loss after mini-batch  6001: 43.942\n",
      "Loss after mini-batch  7001: 46.107\n",
      "Loss after mini-batch  8001: 48.070\n",
      "Loss after mini-batch  9001: 49.744\n",
      "Loss after mini-batch 10001: 44.171\n",
      "Loss after mini-batch 11001: 45.087\n",
      "Loss after mini-batch 12001: 69.970\n",
      "Loss after mini-batch 13001: 83.329\n",
      "Loss after mini-batch 14001: 123.782\n",
      "Loss after mini-batch 15001: 52.020\n",
      "Loss after mini-batch 16001: 46.966\n",
      "Loss after mini-batch 17001: 73.061\n",
      "Loss after mini-batch 18001: 64.479\n",
      "Loss after mini-batch 19001: 88.230\n",
      "Loss after mini-batch 20001: 95.625\n",
      "Test loss after mini-batch     1: 0.005\n",
      "Test loss after mini-batch  1001: 122.333\n",
      "Test loss after mini-batch  2001: 131.530\n",
      "Test loss after mini-batch  3001: 64.492\n",
      "Test loss after mini-batch  4001: 115.610\n",
      "Test loss after mini-batch  5001: 101.831\n",
      "Test loss after mini-batch  6001: 136.603\n",
      "Test loss after mini-batch  7001: 84.533\n",
      "Test loss after mini-batch  8001: 96.861\n",
      "Test loss after mini-batch  9001: 77.062\n",
      "Test loss after mini-batch 10001: 102.438\n",
      "Test loss after mini-batch 11001: 98.807\n",
      "Test loss after mini-batch 12001: 134.943\n",
      "Test loss after mini-batch 13001: 152.204\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "for epoch in range(0, 5): # 5 epochs at maximum\n",
    "  \n",
    "  # Print epoch\n",
    "  print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "  # Set current loss value\n",
    "  current_loss = 0.0\n",
    "  test_loss = 0.0\n",
    "  \n",
    "  # Iterate over the DataLoader for training data\n",
    "  for i, data in enumerate(train_dataloader, 0):\n",
    "    \n",
    "    \"\"\" # Get and prepare inputs\n",
    "    inputs, targets = data\n",
    "    inputs, targets = inputs.float(), targets.float()\n",
    "    targets = targets.reshape((targets.shape[0], 1))\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Perform forward pass\n",
    "    outputs = mlp(inputs)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function(outputs, targets)\n",
    "    \n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Perform optimization\n",
    "    optimizer.step() \"\"\"\n",
    "    \n",
    "    # Print statistics\n",
    "    current_loss += train(data)\n",
    "    if i % 1000 == 0:\n",
    "        print('Loss after mini-batch %5d: %.3f' %\n",
    "              (i + 1, current_loss / 500))\n",
    "        current_loss = 0.0\n",
    "  for i, data in enumerate(test_dataloader, 0):\n",
    "    # Print statistics\n",
    "    test_loss += test(data)\n",
    "    if i % 1000 == 0:\n",
    "        print('Test loss after mini-batch %5d: %.3f' %\n",
    "              (i + 1, test_loss / 500))\n",
    "        test_loss = 0.0\n",
    "        \n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            #X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=15, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Master\\Machine-learning-and-sensor-technology\\Assignment 3\\Assignment 3.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mlp(test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Master\\Machine-learning-and-sensor-technology\\Assignment 3\\Assignment 3.ipynb Cell 34\u001b[0m in \u001b[0;36mMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#Y103sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#Y103sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m   \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#Y103sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m    Forward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#Y103sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m  '''\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#Y103sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not function"
     ]
    }
   ],
   "source": [
    "mlp(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to mlp_model_torch.pth\n"
     ]
    }
   ],
   "source": [
    "mlp.eval()\n",
    "torch.save(mlp.state_dict(), \"mlp_model_torch.pth\")\n",
    "print(\"Saved PyTorch Model State to mlp_model_torch.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try RNN with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=15, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(15, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X, y\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            #X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Master\\Machine-learning-and-sensor-technology\\Assignment 3\\Assignment 3.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     test(test_dataloader, model, loss_fn)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Master\\Machine-learning-and-sensor-technology\\Assignment 3\\Assignment 3.ipynb Cell 37\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X, y \u001b[39m=\u001b[39m X, y\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Compute prediction error\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pred \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Master\\Machine-learning-and-sensor-technology\\Assignment 3\\Assignment 3.ipynb Cell 37\u001b[0m in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_relu_stack(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Master/Machine-learning-and-sensor-technology/Assignment%203/Assignment%203.ipynb#X41sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize our data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(100)\n",
    "y = np.sin(x) * np.power(x,3) + 3*x + np.random.rand(100)*0.8\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70368832 0.20537755 0.34578633 0.00618444 0.54674924 0.10583509\n",
      " 0.81642895 0.53908583 0.05486176 0.78216681 0.12813997 0.46288816\n",
      " 0.11167669 0.65574317 0.78008015 0.41806401 0.91147067 0.84149513\n",
      " 0.85103715 0.11550736 0.57437397 0.9539109  0.70323876 0.38046883\n",
      " 0.50701823 0.93869334 0.28359412 0.50911558 0.81402274 0.56245373\n",
      " 0.79671655 0.01640915 0.35924399 0.95557992 0.66251308 0.75757289\n",
      " 0.26925195 0.24594452 0.53299768 0.51730169 0.36105009 0.74775881\n",
      " 0.79250962 0.54755019 0.59242947 0.38182256 0.54493402 0.12529279\n",
      " 0.90773917 0.93045576 0.73395483 0.91326743 0.70717395 0.45436712\n",
      " 0.9092302  0.39264724 0.84211439 0.94320655 0.91890966 0.62616743\n",
      " 0.186682   0.4876609  0.7827188  0.36827864 0.86295424 0.33335232\n",
      " 0.44212418 0.28517396 0.61619583 0.37994823 0.28305465 0.32147539\n",
      " 0.84954417 0.90517096 0.8929423  0.95200309 0.29540049 0.56905443\n",
      " 0.04168883 0.98419328 0.78968602 0.12326033 0.9903954  0.03810497\n",
      " 0.94231401 0.00655177 0.65714115 0.18602914 0.55884098 0.84805446\n",
      " 0.63987093 0.22703124 0.19304912 0.18032047 0.90770156 0.52501829\n",
      " 0.21959614 0.392993   0.35154829 0.57923396] [2.83297519 1.34603029 1.72093742 0.60778575 2.41339336 1.05716699\n",
      " 2.9299901  2.28417969 0.2962151  2.83386767 0.73618089 1.55033258\n",
      " 1.10172424 2.40479185 3.08818762 1.52855191 3.66249284 3.45499875\n",
      " 3.57384761 0.70547701 2.07897554 3.57093193 2.63035161 1.58212852\n",
      " 1.69508583 3.85170257 1.20786426 1.98762798 3.27827139 2.24573221\n",
      " 3.0466345  0.13503829 1.20204764 4.32399123 2.70925532 2.70420498\n",
      " 1.56339174 1.18712285 2.0750346  1.80563497 1.5559993  3.25921609\n",
      " 3.40728333 2.35701449 2.33713596 1.65315473 2.41351189 0.43261319\n",
      " 3.50515441 3.43913357 2.58312429 4.08001109 2.84433507 1.85376403\n",
      " 3.68257335 1.68730959 3.49466183 4.11594285 3.736961   2.41228814\n",
      " 1.08922198 1.9840009  2.69838547 1.17097814 3.57098843 1.57863563\n",
      " 2.01474164 1.66019348 2.42320765 1.5734973  1.57325433 1.63621485\n",
      " 3.09169946 3.95752598 3.55094987 3.82125803 1.37124645 2.34646881\n",
      " 0.27366766 4.27536748 3.23907268 1.00424941 4.19406869 0.90648322\n",
      " 3.87049657 0.81611691 2.22390951 0.74226038 1.99273203 3.48867704\n",
      " 2.65495984 0.79573386 0.94933291 1.1433637  4.09081695 1.87462765\n",
      " 1.18618193 1.20406456 1.5114051  2.3205095 ]\n"
     ]
    }
   ],
   "source": [
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1860],\n",
      "        [0.6906],\n",
      "        [0.4548],\n",
      "        [0.6744],\n",
      "        [0.8927],\n",
      "        [0.1820],\n",
      "        [0.2191],\n",
      "        [0.3992],\n",
      "        [0.5337],\n",
      "        [0.7261],\n",
      "        [0.7891],\n",
      "        [0.4268],\n",
      "        [0.7827],\n",
      "        [0.9019],\n",
      "        [0.7478],\n",
      "        [0.3457],\n",
      "        [0.6754],\n",
      "        [0.6508],\n",
      "        [0.2762],\n",
      "        [0.4080],\n",
      "        [0.9263],\n",
      "        [0.4477],\n",
      "        [0.9933],\n",
      "        [0.5244],\n",
      "        [0.1542],\n",
      "        [0.6760],\n",
      "        [0.6813],\n",
      "        [0.1011],\n",
      "        [0.2423],\n",
      "        [0.9539],\n",
      "        [0.9680],\n",
      "        [0.7626],\n",
      "        [0.2917],\n",
      "        [0.2139],\n",
      "        [0.9316],\n",
      "        [0.2281],\n",
      "        [0.6971],\n",
      "        [0.3119],\n",
      "        [0.7892],\n",
      "        [0.5273],\n",
      "        [0.6609],\n",
      "        [0.6452],\n",
      "        [0.6797],\n",
      "        [0.1833],\n",
      "        [0.1401],\n",
      "        [0.6635],\n",
      "        [0.7804],\n",
      "        [0.2870],\n",
      "        [0.1118],\n",
      "        [0.4706],\n",
      "        [0.4329],\n",
      "        [0.1952],\n",
      "        [0.9398],\n",
      "        [0.6063],\n",
      "        [0.0655],\n",
      "        [0.1111],\n",
      "        [0.4538],\n",
      "        [0.7426],\n",
      "        [0.4541],\n",
      "        [0.2000],\n",
      "        [0.0805],\n",
      "        [0.1979],\n",
      "        [0.1436],\n",
      "        [0.3991],\n",
      "        [0.7698],\n",
      "        [0.1660],\n",
      "        [0.8312],\n",
      "        [0.9676],\n",
      "        [0.9716],\n",
      "        [0.8225],\n",
      "        [0.3459],\n",
      "        [0.8333],\n",
      "        [0.8829],\n",
      "        [0.3465],\n",
      "        [0.5105],\n",
      "        [0.3850],\n",
      "        [0.0262],\n",
      "        [0.4982],\n",
      "        [0.1320],\n",
      "        [0.2815],\n",
      "        [0.2083],\n",
      "        [0.2166],\n",
      "        [0.4372],\n",
      "        [0.8831],\n",
      "        [0.9724],\n",
      "        [0.1280],\n",
      "        [0.6292],\n",
      "        [0.8267],\n",
      "        [0.1218],\n",
      "        [0.3204],\n",
      "        [0.7008],\n",
      "        [0.6253],\n",
      "        [0.3127],\n",
      "        [0.7965],\n",
      "        [0.7885],\n",
      "        [0.6472],\n",
      "        [0.8247],\n",
      "        [0.2256],\n",
      "        [0.8195],\n",
      "        [0.4696]]) tensor([[0.7756],\n",
      "        [2.9271],\n",
      "        [2.0298],\n",
      "        [2.4700],\n",
      "        [3.9835],\n",
      "        [0.7056],\n",
      "        [1.3839],\n",
      "        [1.7813],\n",
      "        [1.7891],\n",
      "        [3.1435],\n",
      "        [3.2972],\n",
      "        [1.9267],\n",
      "        [2.7231],\n",
      "        [3.3774],\n",
      "        [2.9998],\n",
      "        [1.6423],\n",
      "        [2.5053],\n",
      "        [2.3659],\n",
      "        [0.9234],\n",
      "        [1.6582],\n",
      "        [4.1441],\n",
      "        [1.7449],\n",
      "        [4.4624],\n",
      "        [1.6520],\n",
      "        [0.8406],\n",
      "        [2.8207],\n",
      "        [2.9919],\n",
      "        [1.0997],\n",
      "        [1.4324],\n",
      "        [4.2023],\n",
      "        [3.9184],\n",
      "        [3.1410],\n",
      "        [1.4513],\n",
      "        [1.1447],\n",
      "        [3.6323],\n",
      "        [1.3761],\n",
      "        [2.7790],\n",
      "        [1.3387],\n",
      "        [2.7584],\n",
      "        [2.3821],\n",
      "        [2.7867],\n",
      "        [2.4354],\n",
      "        [2.6584],\n",
      "        [0.8376],\n",
      "        [0.4819],\n",
      "        [2.5517],\n",
      "        [2.8711],\n",
      "        [1.1041],\n",
      "        [0.5568],\n",
      "        [2.0387],\n",
      "        [1.9988],\n",
      "        [1.1721],\n",
      "        [3.8724],\n",
      "        [2.2774],\n",
      "        [0.3242],\n",
      "        [1.1235],\n",
      "        [1.4084],\n",
      "        [2.7223],\n",
      "        [2.0299],\n",
      "        [1.0464],\n",
      "        [0.4643],\n",
      "        [1.3650],\n",
      "        [0.9393],\n",
      "        [1.6815],\n",
      "        [2.7407],\n",
      "        [0.9053],\n",
      "        [3.1890],\n",
      "        [4.4389],\n",
      "        [3.7944],\n",
      "        [3.5615],\n",
      "        [1.3123],\n",
      "        [3.5134],\n",
      "        [3.7902],\n",
      "        [1.4842],\n",
      "        [1.6657],\n",
      "        [1.8996],\n",
      "        [0.8007],\n",
      "        [1.8686],\n",
      "        [0.5078],\n",
      "        [1.3971],\n",
      "        [1.1290],\n",
      "        [0.7409],\n",
      "        [2.0969],\n",
      "        [3.5397],\n",
      "        [4.4100],\n",
      "        [1.1230],\n",
      "        [2.7544],\n",
      "        [3.5036],\n",
      "        [0.6822],\n",
      "        [1.0952],\n",
      "        [3.0813],\n",
      "        [2.5683],\n",
      "        [1.3777],\n",
      "        [3.0922],\n",
      "        [3.0212],\n",
      "        [2.7650],\n",
      "        [3.6507],\n",
      "        [1.0836],\n",
      "        [3.2506],\n",
      "        [1.7409]])\n"
     ]
    }
   ],
   "source": [
    "# convert numpy array to tensor in shape of input size\n",
    "x = torch.from_numpy(x.reshape(-1,1)).float()\n",
    "y = torch.from_numpy(y.reshape(-1,1)).float()\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d95096211da4458030c5d20d7fc773d80967e0e0df0f24e7721081e9a522ec2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
